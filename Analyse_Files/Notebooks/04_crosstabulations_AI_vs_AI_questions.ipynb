{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95652542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# SETUP BLOCK \n",
    "# ==========================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# ---- Imports from project files ----\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from Helper_funtions import (\n",
    "    clean_up_subjects,\n",
    "    calculate_true_false_score,\n",
    "    calculate_Internet_terms_understanding_score,\n",
    "    group_internet_understanding\n",
    ")\n",
    "from lists import (\n",
    "    multiple_choice_questions,\n",
    "    likert_mapping,\n",
    "    comparison_pairs_by_AI_questions,\n",
    "    cross_tab_titles_and_colors\n",
    "    \n",
    ")\n",
    "from answer_categories import question_orders\n",
    "\n",
    "# ---- General plot style ----\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "\n",
    "# ---- Data loading ----\n",
    "DATA_FILE = os.path.join(\"..\", \"Data\", \"Fertige Tabelle.xlsx\")\n",
    "df = pd.read_excel(DATA_FILE)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Clean up multi-subject columns\n",
    "for col in [\"Most used subjects\", \"Preferred Subjects\", \"Least preferred Subjects\"]:\n",
    "    if col in df.columns:\n",
    "        df = clean_up_subjects(df, col)\n",
    "\n",
    "# Calculate additional scores\n",
    "if all(q in df.columns for q in [\"True/False_1\", \"True/False_2\"]):\n",
    "    df = calculate_true_false_score(df)\n",
    "\n",
    "if any(col.startswith(\"Internet terms_\") for col in df.columns):\n",
    "    df = calculate_Internet_terms_understanding_score(df)\n",
    "    df = group_internet_understanding(df)\n",
    "\n",
    "print(\"✅ Setup complete – DataFrame loaded and preprocessed\")\n",
    "print(f\"Rows: {len(df)}, Columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8e8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# BUILD COUNT COLUMNS FROM MULTI-CHOICE TEXT\n",
    "# ==========================================\n",
    "\n",
    "def build_count_from_multichoice(df_in: pd.DataFrame, source_col: str, new_col: str) -> pd.DataFrame:\n",
    "   \n",
    "    if source_col not in df_in.columns:\n",
    "        print(f\"⚠️ Source column '{source_col}' not in DataFrame; cannot build '{new_col}'.\")\n",
    "        return df_in\n",
    "\n",
    "    def _count_items(x):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        s = str(x).strip()\n",
    "        if s == \"\" or s.lower() in {\"nan\", \"none\"}:\n",
    "            return np.nan\n",
    "\n",
    "        s = s.replace(\";\", \",\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").replace('\"', \"\")\n",
    "        parts = [p.strip() for p in s.split(\",\")]\n",
    "        parts = [p for p in parts if p]\n",
    "\n",
    "        seen = set()\n",
    "        unique_parts = []\n",
    "        for p in parts:\n",
    "            if p not in seen:\n",
    "                seen.add(p)\n",
    "                unique_parts.append(p)\n",
    "        return len(unique_parts) if unique_parts else np.nan\n",
    "\n",
    "    df_in[new_col] = df_in[source_col].apply(_count_items).astype(\"Int64\")\n",
    "    print(f\"✅ Built count column '{new_col}' from '{source_col}'.\")\n",
    "    return df_in\n",
    "\n",
    "\n",
    "df = build_count_from_multichoice(df, \"Reasons to use AI\", \"Reasons to use AI (Count)\")\n",
    "\n",
    "df = build_count_from_multichoice(df, \"Purposes to use AI\", \"Purposes to use AI (Count)\")\n",
    "\n",
    "\n",
    "try:\n",
    "    min_c = int(df[\"Reasons to use AI (Count)\"].min())\n",
    "    max_c = int(df[\"Reasons to use AI (Count)\"].max())\n",
    "    if \"question_orders\" in globals():\n",
    "        question_orders[\"Reasons to use AI (Count)\"] = list(range(min_c, max_c + 1))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2694b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ bar charts ------\n",
    "\n",
    "def build_pair_dataframe(df_in: pd.DataFrame, left: str, right: str) -> pd.DataFrame:\n",
    "    left_is_multi  = left  in multiple_choice_questions\n",
    "    right_is_multi = right in multiple_choice_questions\n",
    "\n",
    "    tmp = df_in[[left, right]].dropna().copy()\n",
    "\n",
    "    if left_is_multi:\n",
    "        tmp[left] = tmp[left].astype(str).str.split(\",\")\n",
    "        tmp = tmp.explode(left)\n",
    "    else:\n",
    "        tmp[left] = tmp[left].astype(str).str.strip()\n",
    "\n",
    "    if right_is_multi:\n",
    "        tmp[right] = tmp[right].astype(str).str.split(\",\")\n",
    "        tmp = tmp.explode(right)\n",
    "    else:\n",
    "        tmp[right] = tmp[right].astype(str).str.strip()\n",
    "\n",
    "    # strip after explode & drop empties\n",
    "    tmp[left]  = tmp[left].astype(str).str.strip()\n",
    "    tmp[right] = tmp[right].astype(str).str.strip()\n",
    "    tmp = tmp[(tmp[left] != \"\") & (tmp[right] != \"\")]\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def order_rows_cols(ct: pd.DataFrame, rows_key: str, cols_key: str) -> pd.DataFrame:\n",
    "    # columns\n",
    "    if cols_key in question_orders:\n",
    "        col_order = [v for v in question_orders[cols_key] if v in ct.columns]\n",
    "        remaining_cols = [v for v in ct.columns if v not in col_order]\n",
    "        ct = ct[col_order + remaining_cols]\n",
    "    else:\n",
    "        try:\n",
    "            ct = ct[sorted(ct.columns, key=lambda x: float(x))]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # rows\n",
    "    if rows_key in question_orders:\n",
    "        row_order = [v for v in question_orders[rows_key] if v in ct.index]\n",
    "        remaining_rows = [v for v in ct.index if v not in row_order]\n",
    "        ct = ct.reindex(row_order + remaining_rows)\n",
    "    else:\n",
    "        try:\n",
    "            ct.index = pd.to_numeric(ct.index)\n",
    "            ct = ct.sort_index()\n",
    "        except Exception:\n",
    "            pass\n",
    "    return ct\n",
    "\n",
    "\n",
    "for base_question, compare_list in comparison_pairs_by_AI_questions.items():\n",
    "    for compare_question in compare_list:\n",
    "        # prepare data\n",
    "        data = build_pair_dataframe(df, base_question, compare_question)\n",
    "        if data.empty:\n",
    "            print(f\"⚠️ No overlapping data for '{base_question}' × '{compare_question}'. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # crosstabulations\n",
    "        ct = pd.crosstab(data[base_question], data[compare_question])\n",
    "\n",
    "        # check for same labels\n",
    "        if not ct.columns.is_unique:\n",
    "            ct = ct.T.groupby(level=0).sum().T\n",
    "        if not ct.index.is_unique:\n",
    "            ct = ct.groupby(level=0).sum()\n",
    "\n",
    "        # order\n",
    "        ct = order_rows_cols(ct, rows_key=base_question, cols_key=compare_question)\n",
    "\n",
    "        # table output\n",
    "        print(f\"\\n📊 {compare_question} within each {base_question} (rows sum to 100%)\")\n",
    "        print(\"Counts:\\n\", ct)\n",
    "\n",
    "        # custom titles\n",
    "        title_and_colors = cross_tab_titles_and_colors.get(\n",
    "            (base_question, compare_question),\n",
    "            [f\"{compare_question} within each {base_question} (100% stacked)\"]\n",
    "        )\n",
    "        plot_title = title_and_colors[0]\n",
    "\n",
    "        # blue palette in 5 shades\n",
    "        blue_palette = sns.color_palette(\"Blues\", n_colors=5)\n",
    "\n",
    "        if len(title_and_colors) > 1:\n",
    "            colors = title_and_colors[1:]\n",
    "            ax = ct.plot(kind=\"bar\", stacked=True, figsize=(10, 6), color=colors, width=0.9)\n",
    "        else:\n",
    "            ax = ct.plot(kind=\"bar\", stacked=True, figsize=(10, 6), color=blue_palette, width=0.9)\n",
    "        \n",
    "        ax.set_title(plot_title)\n",
    "        ax.set_ylabel(\"Percentage (%)\")\n",
    "        ax.set_xlabel(base_question)\n",
    "        ax.set_ylim(0, 100)\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.legend(title=compare_question, bbox_to_anchor=(1.01, 1), loc=\"upper left\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17c5c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# SIGNIFICANCE TESTS for AI×AI crosstabs\n",
    "# ==========================================\n",
    "\n",
    "def cramers_v_corrected(chi2, ct):\n",
    "    \"\"\"Bias-corrected Cramér’s V (Bergsma, 2013).\"\"\"\n",
    "    n = ct.values.sum()\n",
    "    if n == 0:\n",
    "        return np.nan\n",
    "    r, k = ct.shape\n",
    "    phi2 = chi2 / n\n",
    "    if n > 1:\n",
    "        phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
    "        rcorr = r - ((r - 1) ** 2) / (n - 1)\n",
    "        kcorr = k - ((k - 1) ** 2) / (n - 1)\n",
    "    else:\n",
    "        phi2corr, rcorr, kcorr = np.nan, r, k\n",
    "    denom = min(rcorr - 1, kcorr - 1)\n",
    "    return np.sqrt(phi2corr / denom) if denom > 0 else np.nan\n",
    "\n",
    "ai_ai_tests = []\n",
    "\n",
    "# main loop\n",
    "for base_question, compare_list in comparison_pairs_by_AI_questions.items():\n",
    "    for compare_question in compare_list:\n",
    "        try:\n",
    "            # prepare paired data \n",
    "            data = build_pair_dataframe(df, base_question, compare_question)\n",
    "            if data.empty:\n",
    "                print(f\"⚠️ No overlapping data for '{base_question}' × '{compare_question}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # contingency table\n",
    "            ct = pd.crosstab(data[base_question], data[compare_question])\n",
    "\n",
    "            # ensure unique index/columns\n",
    "            if not ct.columns.is_unique:\n",
    "                ct = ct.T.groupby(level=0).sum().T\n",
    "            if not ct.index.is_unique:\n",
    "                ct = ct.groupby(level=0).sum()\n",
    "\n",
    "            # order rows/cols \n",
    "            ct = order_rows_cols(ct, rows_key=base_question, cols_key=compare_question)\n",
    "\n",
    "            # drop zero-sum rows/cols (safety)\n",
    "            ct = ct.loc[ct.sum(axis=1) > 0, ct.sum(axis=0) > 0]\n",
    "            if ct.shape[0] < 2 or ct.shape[1] < 2:\n",
    "                print(f\"⚠️ Contingency too small for '{base_question}' × '{compare_question}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Chi-square test\n",
    "            chi2, p, dof, expected = chi2_contingency(ct.values, correction=False)\n",
    "            expected_df = pd.DataFrame(expected, index=ct.index, columns=ct.columns)\n",
    "\n",
    "            # effect size + assumption checks\n",
    "            v = cramers_v_corrected(chi2, ct)\n",
    "            min_exp = float(expected_df.values.min())\n",
    "            prop_lt5 = float((expected_df.values < 5).mean())\n",
    "            n_total = int(ct.values.sum())\n",
    "\n",
    "            # console summary\n",
    "            print(f\"\\n🧪 Significance: {base_question} × {compare_question}\")\n",
    "            print(f\"χ²({dof}) = {chi2:.3f}, p = {p:.4f},  Cramér’s V = {v:.3f}\")\n",
    "            print(f\"Assumptions: min(expected) = {min_exp:.2f}, %cells<5 = {100*prop_lt5:.1f}%\")\n",
    "\n",
    "            # collect results\n",
    "            ai_ai_tests.append({\n",
    "                \"base_question\": base_question,\n",
    "                \"compare_question\": compare_question,\n",
    "                \"n\": n_total,\n",
    "                \"rows\": ct.shape[0],\n",
    "                \"cols\": ct.shape[1],\n",
    "                \"chi2\": chi2,\n",
    "                \"dof\": dof,\n",
    "                \"p\": p,\n",
    "                \"cramers_v\": v,\n",
    "                \"min_expected\": min_exp,\n",
    "                \"prop_expected_lt5\": prop_lt5,\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Failed significance for {base_question} × {compare_question}: {e}\")\n",
    "\n",
    "# results dataframe\n",
    "ai_ai_tests_df = (\n",
    "    pd.DataFrame(\n",
    "        ai_ai_tests,\n",
    "        columns=[\"base_question\",\"compare_question\",\"n\",\"rows\",\"cols\",\n",
    "                 \"chi2\",\"dof\",\"p\",\"cramers_v\",\"min_expected\",\"prop_expected_lt5\"]\n",
    "    )\n",
    "    .sort_values([\"base_question\",\"compare_question\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"\\n✅ AI×AI significance finished.\")\n",
    "print(f\"Total tests: {len(ai_ai_tests_df)}\")\n",
    "\n",
    "# optional Excel export\n",
    "EXPORT = True\n",
    "EXPORT_PATH = os.path.join(\"..\", \"Data/test_results\", \"ai_ai_crosstabs_significance.xlsx\")\n",
    "if EXPORT:\n",
    "    with pd.ExcelWriter(EXPORT_PATH, engine=\"xlsxwriter\") as writer:\n",
    "        ai_ai_tests_df.to_excel(writer, index=False, sheet_name=\"chi2_summary\")\n",
    "    print(f\"💾 Exported to: {EXPORT_PATH}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59442d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# SPEARMAN TREND TESTS (ordinal × ordinal)\n",
    "# ==========================================\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# flexible mapping to numeric\n",
    "def map_to_numeric_flexible(series: pd.Series, varname: str) -> pd.Series:\n",
    "    s = series.astype(str)\n",
    "\n",
    "    # project-specific likert mapping\n",
    "    if 'likert_mapping' in globals() and varname in likert_mapping:\n",
    "        mp = {str(k): v for k, v in likert_mapping[varname].items()}\n",
    "        return s.map(mp)\n",
    "\n",
    "    # project-specific category order\n",
    "    if 'question_orders' in globals() and varname in question_orders:\n",
    "        order = [str(x) for x in question_orders[varname]]\n",
    "        mapper = {cat: i for i, cat in enumerate(order)}\n",
    "        out = s.map(mapper)\n",
    "        if out.notna().any():\n",
    "            return out.astype(float)\n",
    "\n",
    "    # orders\n",
    "    orders = [\n",
    "        [\"Nie\",\"Selten\",\"Manchmal\",\"Oft\",\"Sehr oft\"],\n",
    "        [\"Nie\",\"Seltener\",\"Etwa 1 Mal pro Woche\",\"Mehrmals pro Woche\",\"Täglich\"],\n",
    "        [\"Gar nicht\",\"Eher wenig\",\"Teils/teils\",\"Eher gut\",\"Sehr gut\"],\n",
    "        [\"Gar nicht verlässlich\",\"Wenig verlässlich\",\"Unsicher / Ich habe keine Meinung\",\n",
    "         \"Teils/teils\",\"Eher verlässlich\",\"Sehr verlässlich\"],\n",
    "        [\"Kein Verständnis\",\"Schlechtes Verständnis\",\"Mittelmässiges Verständnis\",\n",
    "         \"Gutes Verständnis\",\"Völliges Verständnis\"],\n",
    "        [\"Stört mich sehr\",\"Stört mich ein wenig\",\"Neutral / Mir egal\",\"Finde ich gut\"],\n",
    "    ]\n",
    "    su = set(s.unique())\n",
    "    for order in orders:\n",
    "        if su.issubset(set(order)):\n",
    "            mapper = {cat: i for i, cat in enumerate(order)}\n",
    "            return s.map(mapper).astype(float)\n",
    "\n",
    "    # fallback, numeric conversion\n",
    "    return pd.to_numeric(series, errors=\"coerce\")\n",
    "\n",
    "\n",
    "# order categories\n",
    "def make_ordered_group(series: pd.Series, varname: str) -> pd.Categorical:\n",
    "    s = series.astype(str)\n",
    "    uniq = sorted(s.unique())\n",
    "\n",
    "    # project-specific order if available\n",
    "    if 'question_orders' in globals() and varname in question_orders:\n",
    "        desired = [str(x) for x in question_orders[varname]]\n",
    "        cats = [c for c in desired if c in s.unique()] + [c for c in uniq if c not in desired]\n",
    "    else:\n",
    "        # try numeric sort, fallback alphabetic\n",
    "        try:\n",
    "            nums = sorted({float(x) for x in s.unique()})\n",
    "            cats = [str(int(x)) if float(x).is_integer() else str(x) for x in nums]\n",
    "        except Exception:\n",
    "            cats = uniq\n",
    "    return pd.Categorical(s, categories=cats, ordered=True)\n",
    "\n",
    "\n",
    "#run Spearman test \n",
    "def run_spearman_pair(df_in: pd.DataFrame, x: str, y: str):\n",
    "    # skip text MC\n",
    "    if 'multiple_choice_questions' in globals():\n",
    "        if (x in multiple_choice_questions) and \"(Count)\" not in x:\n",
    "            print(f\"Spearman skipped for {x} × {y}: '{x}' is MC-text, use (Count) column.\")\n",
    "            return None\n",
    "        if (y in multiple_choice_questions) and \"(Count)\" not in y:\n",
    "            print(f\"Spearman skipped for {x} × {y}: '{y}' is MC-text, use (Count) column.\")\n",
    "            return None\n",
    "\n",
    "    X_raw = df_in[x]\n",
    "    Y_raw = df_in[y]\n",
    "\n",
    "    # map to numeric\n",
    "    X = map_to_numeric_flexible(X_raw, x)\n",
    "    Y = map_to_numeric_flexible(Y_raw, y)\n",
    "\n",
    "    # build data\n",
    "    data = pd.DataFrame({x: X, y: Y, f\"{x}__raw\": X_raw, f\"{y}__raw\": Y_raw}).dropna()\n",
    "    if data.empty:\n",
    "        print(f\"No data for Spearman {x} × {y}.\")\n",
    "        return None\n",
    "\n",
    "    # Spearman test\n",
    "    rho, p = spearmanr(data[x].values, data[y].values)\n",
    "\n",
    "    # medians grouped by x\n",
    "    x_lab = make_ordered_group(data[f\"{x}__raw\"], x)\n",
    "    data[\"_x_lab\"] = x_lab\n",
    "    median_by_x = data.groupby(\"_x_lab\", observed=True)[y].median().reset_index()\n",
    "    median_by_x.columns = [x, f\"median_{y}\"]\n",
    "\n",
    "    # medians grouped by y\n",
    "    y_lab = make_ordered_group(data[f\"{y}__raw\"], y)\n",
    "    data[\"_y_lab\"] = y_lab\n",
    "    median_by_y = data.groupby(\"_y_lab\", observed=True)[x].median().reset_index()\n",
    "    median_by_y.columns = [y, f\"median_{x}\"]\n",
    "\n",
    "    res = {\n",
    "        \"x\": x, \"y\": y,\n",
    "        \"n\": int(len(data)),\n",
    "        \"rho\": float(rho),\n",
    "        \"p\": float(p),\n",
    "        \"direction\": \"positive\" if rho > 0 else (\"negative\" if rho < 0 else \"zero\"),\n",
    "        \"median_by_x\": median_by_x,\n",
    "        \"median_by_y\": median_by_y,\n",
    "    }\n",
    "    print(f\"Spearman {x} × {y}: ρ={rho:.3f}, p={p:.4g}, n={len(data)}\")\n",
    "    return res\n",
    "\n",
    "\n",
    "# run multiple pairs\n",
    "def run_spearman_pairs(df_in: pd.DataFrame, pairs, export_path=None):\n",
    "    all_rows = []\n",
    "    all_median_x = []\n",
    "    all_median_y = []\n",
    "\n",
    "    for (x, y) in pairs:\n",
    "        try:\n",
    "            out = run_spearman_pair(df_in, x, y)\n",
    "            if out is None:\n",
    "                continue\n",
    "            all_rows.append({\n",
    "                \"x\": out[\"x\"], \"y\": out[\"y\"],\n",
    "                \"n\": out[\"n\"], \"rho\": out[\"rho\"], \"p\": out[\"p\"],\n",
    "                \"direction\": out[\"direction\"]\n",
    "            })\n",
    "            tmpx = out[\"median_by_x\"].copy(); tmpx[\"pair\"] = f\"{x} × {y}\"\n",
    "            tmpy = out[\"median_by_y\"].copy(); tmpy[\"pair\"] = f\"{x} × {y}\"\n",
    "            all_median_x.append(tmpx); all_median_y.append(tmpy)\n",
    "        except Exception as e:\n",
    "            print(f\"Spearman failed for {x} × {y}: {e}\")\n",
    "\n",
    "    # build outputs\n",
    "    summary = (pd.DataFrame(all_rows, columns=[\"x\",\"y\",\"n\",\"rho\",\"p\",\"direction\"])\n",
    "               .sort_values([\"x\",\"y\"]).reset_index(drop=True))\n",
    "    med_x = pd.concat(all_median_x, ignore_index=True) if all_median_x else pd.DataFrame()\n",
    "    med_y = pd.concat(all_median_y, ignore_index=True) if all_median_y else pd.DataFrame()\n",
    "\n",
    "    print(\"\\n✅ Spearman run finished.\")\n",
    "    if len(summary):\n",
    "        print(summary.to_string(index=False))\n",
    "    else:\n",
    "        print(\"No successful pairs.\")\n",
    "\n",
    "    # export results\n",
    "    if export_path:\n",
    "        with pd.ExcelWriter(export_path, engine=\"xlsxwriter\") as writer:\n",
    "            summary.to_excel(writer, index=False, sheet_name=\"spearman_summary\")\n",
    "            if not med_x.empty: med_x.to_excel(writer, index=False, sheet_name=\"medians_by_x\")\n",
    "            if not med_y.empty: med_y.to_excel(writer, index=False, sheet_name=\"medians_by_y\")\n",
    "        print(f\"Exported to: {export_path}\")\n",
    "\n",
    "    return summary, med_x, med_y\n",
    "\n",
    "spearman_pairs = [\n",
    "    (\"Use AI school and freetime\", \"Usefullness AI\"),\n",
    "    (\"Use AI school and freetime\", \"Reliability AI\"),\n",
    "    (\"Use AI school and freetime\", \"Mates using AI\"),\n",
    "    (\"Use AI school and freetime\", \"Deal with AI\"),\n",
    "    (\"Reliability AI\", \"Teachers preparing lessons\"),\n",
    "    (\"Reliability AI\", \"Teachers giving grades\"),\n",
    "    (\"Frequency use of AI_school\", \"Help of AI\"),\n",
    "    (\"Frequency use of AI_school\", \"Reasons to use AI (Count)\"),\n",
    "    (\"Frequency use of AI_school\", \"Purposes to use AI (Count)\")\n",
    "]\n",
    "\n",
    "summary_df, medx_df, medy_df = run_spearman_pairs(\n",
    "    df, spearman_pairs,\n",
    "    export_path=os.path.join(\"..\",\"Data/test_results\",\"spearman_results.xlsx\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0934fc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PAIRWISE 2×2 TESTS (auto Fisher or Chi-square)\n",
    "# ============================================================\n",
    "\n",
    "from itertools import combinations\n",
    "from scipy.stats import fisher_exact\n",
    "\n",
    "\n",
    "# helpers, fallbacks \n",
    "\n",
    "if 'build_pair_dataframe' not in globals():\n",
    "    def build_pair_dataframe(df_in: pd.DataFrame, left: str, right: str) -> pd.DataFrame:\n",
    "        \"\"\"two-column frame, explode multi-choice, strip blanks.\"\"\"\n",
    "        left_is_multi  = left  in multiple_choice_questions\n",
    "        right_is_multi = right in multiple_choice_questions\n",
    "        tmp = df_in[[left, right]].dropna().copy()\n",
    "        if left_is_multi:\n",
    "            tmp[left] = tmp[left].astype(str).str.split(\",\"); tmp = tmp.explode(left)\n",
    "        else:\n",
    "            tmp[left] = tmp[left].astype(str).str.strip()\n",
    "        if right_is_multi:\n",
    "            tmp[right] = tmp[right].astype(str).str.split(\",\"); tmp = tmp.explode(right)\n",
    "        else:\n",
    "            tmp[right] = tmp[right].astype(str).str.strip()\n",
    "        tmp[left]  = tmp[left].astype(str).str.strip()\n",
    "        tmp[right] = tmp[right].astype(str).str.strip()\n",
    "        tmp = tmp[(tmp[left] != \"\") & (tmp[right] != \"\")]\n",
    "        return tmp\n",
    "\n",
    "if 'order_rows_cols' not in globals():\n",
    "    def order_rows_cols(ct: pd.DataFrame, rows_key: str, cols_key: str) -> pd.DataFrame:\n",
    "        \"\"\"Order rows/cols by project-defined order if available; else try numeric, else keep.\"\"\"\n",
    "        # columns\n",
    "        if 'question_orders' in globals() and cols_key in question_orders:\n",
    "            want = [v for v in question_orders[cols_key] if v in ct.columns]\n",
    "            rest = [v for v in ct.columns if v not in want]\n",
    "            ct = ct[want + rest] if want else ct\n",
    "        else:\n",
    "            try: ct = ct[sorted(ct.columns, key=lambda x: float(x))]\n",
    "            except: pass\n",
    "        # rows\n",
    "        if 'question_orders' in globals() and rows_key in question_orders:\n",
    "            want = [v for v in question_orders[rows_key] if v in ct.index]\n",
    "            rest = [v for v in ct.index if v not in want]\n",
    "            ct = ct.reindex(want + rest)\n",
    "        else:\n",
    "            try:\n",
    "                ct.index = pd.to_numeric(ct.index); ct = ct.sort_index()\n",
    "            except: pass\n",
    "        return ct\n",
    "\n",
    "def test_2x2(a, b, c, d, alternative=\"two-sided\", prefer=\"auto\", yates=False):\n",
    "    \"\"\"\n",
    "    Perform a 2×2 test\n",
    "    returns dict with method, p, chi2, dof, odds_ratio, min_expected\n",
    "    skips when margins are degenerate\n",
    "    \"\"\"\n",
    "    table = np.array([[int(a), int(b)], [int(c), int(d)]], dtype=int)\n",
    "\n",
    "    # skip if any row/column sum is zero\n",
    "    row_sums = table.sum(axis=1)\n",
    "    col_sums = table.sum(axis=0)\n",
    "    if (row_sums[0] == 0) or (row_sums[1] == 0) or (col_sums[0] == 0) or (col_sums[1] == 0):\n",
    "        return {\n",
    "            \"method\": \"skip\", \"reason\": \"degenerate margins (zero row/column total)\",\n",
    "            \"p\": np.nan, \"chi2\": np.nan, \"dof\": 1, \"odds_ratio\": np.nan, \"min_expected\": 0.0,\n",
    "            \"table\": table,\n",
    "        }\n",
    "\n",
    "    # expected counts\n",
    "    N = table.sum()\n",
    "    expected = np.outer(row_sums, col_sums) / N\n",
    "    min_exp = expected.min()\n",
    "\n",
    "    # fisher for low expectations, else Chi-square\n",
    "    method = \"chi2\"\n",
    "    if prefer == \"fisher\" or (prefer == \"auto\" and (min_exp < 5 or (table == 0).any())):\n",
    "        method = \"fisher\"\n",
    "\n",
    "    if method == \"fisher\":\n",
    "        orat, p = fisher_exact(table, alternative=alternative)\n",
    "        chi2_val, dof = np.nan, 1\n",
    "        odds_ratio = float(orat) if np.isfinite(orat) else np.nan\n",
    "    else:\n",
    "        try:\n",
    "            chi2_val, p, dof, _ = chi2_contingency(table, correction=yates)\n",
    "        except Exception:\n",
    "            # fallback to fisher if chi-square fails\n",
    "            orat, p = fisher_exact(table, alternative=alternative)\n",
    "            chi2_val, dof = np.nan, 1\n",
    "            odds_ratio = float(orat) if np.isfinite(orat) else np.nan\n",
    "            return {\n",
    "                \"method\": \"fisher\", \"p\": float(p), \"chi2\": float(chi2_val), \"dof\": int(dof),\n",
    "                \"odds_ratio\": float(odds_ratio), \"min_expected\": float(min_exp), \"table\": table,\n",
    "            }\n",
    "        # odds ratio correction\n",
    "        a_, b_, c_, d_ = table.astype(float).ravel()\n",
    "        if 0 in (a_, b_, c_, d_):\n",
    "            a_, b_, c_, d_ = a_+0.5, b_+0.5, c_+0.5, d_+0.5\n",
    "        odds_ratio = (a_ * d_) / (b_ * c_)\n",
    "\n",
    "    return {\n",
    "        \"method\": method, \"p\": float(p), \"chi2\": float(chi2_val), \"dof\": int(dof),\n",
    "        \"odds_ratio\": float(odds_ratio), \"min_expected\": float(min_exp), \"table\": table,\n",
    "    }\n",
    "\n",
    "def pairwise_2x2_from_crosstab(ct: pd.DataFrame, row_cat: str,\n",
    "                               prefer=\"auto\", alternative=\"two-sided\", yates=False):\n",
    "    \"\"\"Run pairwise 2×2 tests across all column groups\"\"\"\n",
    "    if row_cat not in ct.index:\n",
    "        raise ValueError(f\"'{row_cat}' not in crosstab index\")\n",
    "    totals = ct.sum(axis=0)\n",
    "    cols = ct.columns.tolist()\n",
    "    rows = []\n",
    "    for g1, g2 in combinations(cols, 2):\n",
    "        a = int(ct.loc[row_cat, g1])    # hits in g1\n",
    "        b = int(totals[g1] - a)         # non-hits in g1\n",
    "        c = int(ct.loc[row_cat, g2])    # hits in g2\n",
    "        d = int(totals[g2] - c)         # non-hits in g2\n",
    "        res = test_2x2(a, b, c, d, alternative=alternative, prefer=prefer, yates=yates)\n",
    "        rows.append({\n",
    "            \"row_cat\": row_cat,\n",
    "            \"g1\": g1, \"g2\": g2,\n",
    "            \"a\": a, \"b\": b, \"c\": c, \"d\": d,\n",
    "            \"n_g1\": int(totals[g1]), \"n_g2\": int(totals[g2]),\n",
    "            \"method\": res[\"method\"], \"p\": res[\"p\"], \"chi2\": res[\"chi2\"], \"dof\": res[\"dof\"],\n",
    "            \"odds_ratio\": res[\"odds_ratio\"], \"min_expected\": res[\"min_expected\"],\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def pairwise_2x2_all_rows(ct: pd.DataFrame, prefer=\"auto\", alternative=\"two-sided\", yates=False):\n",
    "    \"\"\"Apply pairwise_2x2_from_crosstab to every row category\"\"\"\n",
    "    out = []\n",
    "    for rc in ct.index:\n",
    "        df_rc = pairwise_2x2_from_crosstab(ct, rc, prefer=prefer, alternative=alternative, yates=yates)\n",
    "        df_rc.insert(0, \"row_cat_all\", rc)\n",
    "        out.append(df_rc)\n",
    "    return pd.concat(out, ignore_index=True) if out else pd.DataFrame()\n",
    "\n",
    "# build crosstabs + run pairwise 2×2 for each pair ----------\n",
    "def run_pairwise_2x2(df_in: pd.DataFrame, pairs, prefer=\"auto\", yates=False,\n",
    "                     only_show_sig=True, alpha=0.05,\n",
    "                     export=True, export_path=os.path.join(\"..\",\"Data\",\"pairwise_2x2_results.xlsx\")):\n",
    "    \"\"\"build crosstabs for pairs, run pairwise 2×2, print/export results\"\"\"\n",
    "    all_results = []\n",
    "    with pd.ExcelWriter(export_path, engine=\"xlsxwriter\") if export else nullcontext() as writer:\n",
    "        for (left, right) in pairs:\n",
    "            try:\n",
    "                data = build_pair_dataframe(df_in, left, right)\n",
    "                if data.empty:\n",
    "                    print(f\"No data for {left} × {right}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                ct = pd.crosstab(data[left], data[right])\n",
    "\n",
    "                # collapse duplicate labels\n",
    "                if not ct.columns.is_unique: ct = ct.T.groupby(level=0).sum().T\n",
    "                if not ct.index.is_unique:   ct = ct.groupby(level=0).sum()\n",
    "\n",
    "                ct = order_rows_cols(ct, rows_key=left, cols_key=right)\n",
    "\n",
    "                # drop zero-sum columns\n",
    "                ct = ct.loc[ct.sum(axis=1) > 0, ct.sum(axis=0) > 0]\n",
    "                if ct.shape[0] < 1 or ct.shape[1] < 2:\n",
    "                    print(f\"Contingency too small for {left} × {right}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                res = pairwise_2x2_all_rows(ct, prefer=prefer, alternative=\"two-sided\", yates=yates)\n",
    "                if res.empty:\n",
    "                    print(f\"No pairwise results for {left} × {right}.\")\n",
    "                    continue\n",
    "                res.insert(0, \"left_var\", left)\n",
    "                res.insert(1, \"right_var\", right)\n",
    "                all_results.append(res)\n",
    "\n",
    "                # \n",
    "                show = res if not only_show_sig else res[res[\"p\"] < alpha]\n",
    "                print(f\"\\nPairwise 2×2: {left} × {right}  —  tests={len(res)}  (showing {'all' if not only_show_sig else f'p<{alpha}'})\")\n",
    "                if show.empty:\n",
    "                    print(\"  (no significant pairs)\")\n",
    "                else:\n",
    "                    print(show.sort_values(\"p\").head(20).to_string(index=False))\n",
    "\n",
    "                # export sheet per pair\n",
    "                if export:\n",
    "                    sheet_name = (left[:15] + \"×\" + right[:15]).replace(\"/\", \"_\")\n",
    "                    res.to_excel(writer, index=False, sheet_name=sheet_name)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed pairwise 2×2 for {left} × {right}: {e}\")\n",
    "\n",
    "    combined = pd.concat(all_results, ignore_index=True) if all_results else pd.DataFrame()\n",
    "    if export:\n",
    "        print(f\"\\nExported to: {export_path}\")\n",
    "    return combined\n",
    "\n",
    "# Helper for 'with' when export=False\n",
    "from contextlib import contextmanager\n",
    "@contextmanager\n",
    "def nullcontext():\n",
    "    \"\"\"Minimal no-op context manager\"\"\"\n",
    "    yield None\n",
    "\n",
    "# --------- 2x2 pairs ---------\n",
    "pairwise_2x2_pairs = [\n",
    "    (\"Use AI school and freetime\", \"Concerns AI\"),\n",
    "    (\"Used AI\", \"Reliability AI\")\n",
    "]\n",
    "\n",
    "# --------- Run ---------\n",
    "pairwise_results = run_pairwise_2x2(\n",
    "    df, pairwise_2x2_pairs,\n",
    "    prefer=\"auto\",   #fisher or chi2\n",
    "    yates=False,     \n",
    "    only_show_sig=True,\n",
    "    alpha=0.05,\n",
    "    export=True,\n",
    "    export_path=os.path.join(\"..\",\"Data/test_results\",\"pairwise_2x2_results.xlsx\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606112ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual graph\n",
    "\n",
    "usage = [\"Daily\", \"Several times a week\", \"About once a week\", \"Rarely\", \"Never\"]\n",
    "\n",
    "df_under = pd.DataFrame([\n",
    "    [8,14,22,10,2],   # Daily\n",
    "    [7,21,29,7,4],    # Several times a week\n",
    "    [3,6,19,10,0],    # About once a week\n",
    "    [0,12,15,3,1],    # Rarely\n",
    "    [0,5,3,2,0],      # Never\n",
    "], index=usage, columns=[\"Very good\",\"Rather good\",\"Neither good nor bad\",\"Rather little\",\"Not at all\"])\n",
    "\n",
    "df_deal = pd.DataFrame([\n",
    "    [24,26,5,1,0],\n",
    "    [15,35,18,0,0],\n",
    "    [6,16,15,1,0],\n",
    "    [2,12,12,4,1],\n",
    "    [0,6,3,1,0],\n",
    "], index=usage, columns=[\"Very good\",\"Rather good\",\"Neither good nor bad\",\"Rather poor\",\"Poor\"])\n",
    "\n",
    "#  Likert mapping to scores (1–5)\n",
    "map_under = {\n",
    "    \"Very good\": 5, \"Rather good\": 4, \"Neither good nor bad\": 3,\n",
    "    \"Rather little\": 2, \"Not at all\": 1\n",
    "}\n",
    "map_deal  = {\n",
    "    \"Very good\": 5, \"Rather good\": 4, \"Neither good nor bad\": 3,\n",
    "    \"Rather poor\": 2, \"Poor\": 1\n",
    "}\n",
    "\n",
    "def mean_score(df, mapping):\n",
    "    scores = pd.Series(mapping)\n",
    "    return (df.mul(scores, axis=1).sum(axis=1) / df.sum(axis=1))\n",
    "\n",
    "mean_under = mean_score(df_under, map_under)   # Understanding\n",
    "mean_deal  = mean_score(df_deal,  map_deal)    # Dealing with AI\n",
    "\n",
    "# Combine + plot \n",
    "means = pd.DataFrame({\n",
    "    \"Understanding\": mean_under,\n",
    "    \"Dealing with AI\": mean_deal\n",
    "}, index=usage)\n",
    "\n",
    "ax = means.plot(kind=\"bar\", figsize=(9,4), width=0.8)\n",
    "\n",
    "ax.set_title(\"Mean score (1–5): Understanding vs. Dealing with AI by usage\", fontsize=14)\n",
    "ax.set_ylabel(\"Mean score (1–5)\", fontsize=12)\n",
    "ax.set_xlabel(\"AI usage (school + free time)\", fontsize=12)\n",
    "ax.set_ylim(1, 5)\n",
    "plt.xticks(rotation=0, fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.grid(False)\n",
    "ax.legend(title=None, fontsize=11)\n",
    "\n",
    "for p in ax.patches:\n",
    "    h = p.get_height()\n",
    "    ax.annotate(f\"{h:.2f}\", (p.get_x() + p.get_width()/2, h),\n",
    "                ha=\"center\", va=\"bottom\", fontsize=10, xytext=(0,2), textcoords=\"offset points\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(means.round(2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45df3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# INDIVIDUAL GRAPH ---------------------\n",
    "\n",
    "# data by hand\n",
    "df_de = pd.DataFrame(\n",
    "    {\n",
    "        \"Sehr gut\":       [2, 5, 18, 7],\n",
    "        \"Eher gut\":       [13, 33, 37, 6],\n",
    "        \"Neutral\":        [13, 22, 14, 0],\n",
    "        \"Eher schlecht\":  [9,  2,  1,  0],\n",
    "    },\n",
    "    index=[\"Selten\", \"Manchmal\", \"Häufig\", \"Immer\"]\n",
    ")\n",
    "\n",
    "# map english conotation\n",
    "col_map = {\n",
    "    \"Sehr gut\": \"Very good\",\n",
    "    \"Eher gut\": \"Rather good\",\n",
    "    \"Neutral\": \"Neutral\",\n",
    "    \"Eher schlecht\": \"Rather poor\",\n",
    "}\n",
    "index_map = {\n",
    "    \"Selten\": \"Rarely\",\n",
    "    \"Manchmal\": \"Sometimes\",\n",
    "    \"Häufig\": \"Often\",\n",
    "    \"Immer\": \"Always\",\n",
    "}\n",
    "\n",
    "df = df_de.rename(columns=col_map).rename(index=index_map)\n",
    "\n",
    "# make order\n",
    "order = [\"Always\", \"Often\", \"Sometimes\", \"Rarely\"]\n",
    "df = df.reindex(order)\n",
    "\n",
    "# colours\n",
    "colors = [\"#08519c\", \"#3182bd\", \"#6baed6\", \"#c6dbef\"]  # Very good ... Rather poor\n",
    "\n",
    "# plot\n",
    "ax = df.plot(\n",
    "    kind=\"bar\",\n",
    "    stacked=True,\n",
    "    figsize=(10, 6),\n",
    "    width=0.85,\n",
    "    color=colors\n",
    ")\n",
    "\n",
    "ax.set_title(\"Helpfulness of AI tools by frequency of AI usage in school context\", fontsize=14, weight=\"bold\")\n",
    "ax.set_xlabel(\"Frequency of AI usage in school context\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.xticks(rotation=20, ha=\"right\")\n",
    "\n",
    "# legend\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[::-1], labels[::-1], title=\"Help of AI\",\n",
    "          loc=\"upper right\", framealpha=0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ea2141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# configuration\n",
    "COL_FREQ     = \"Frequency use of AI_school\"\n",
    "COL_REASONS  = \"Reasons to use AI\"     \n",
    "COL_PURPOSES = \"Purposes to use AI\"    \n",
    "\n",
    "# helper\n",
    "def count_multiselect_unique(val):\n",
    "    \"\"\"Zählt eindeutige, nicht-leere Auswahloptionen (Komma/Semikolon-getrennt).\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return 0\n",
    "    if isinstance(val, list):\n",
    "        tokens = [str(x).strip() for x in val]\n",
    "    else:\n",
    "        tokens = re.split(r\"\\s*[;,]\\s*\", str(val))\n",
    "        tokens = [t.strip() for t in tokens]\n",
    "    tokens = [t for t in tokens if t]\n",
    "    return len(set(tokens))\n",
    "\n",
    "def pick_freq_order(index_values):\n",
    "    #order\n",
    "    order = [\n",
    "        [\"Always\", \"Often\", \"Sometimes\", \"Rarely\", \"Never\"],\n",
    "        [\"Daily\", \"Several times a week\", \"About once a week\", \"Rarely\", \"Never\"],\n",
    "        [\"Immer\", \"Häufig\", \"Manchmal\", \"Selten\", \"Nie\"],\n",
    "        [\"Täglich\",\"Mehrmals pro Woche\",\"Etwa 1 Mal pro Woche\",\"Seltener\",\"Nie\"]\n",
    "    ]\n",
    "    present = set(index_values)\n",
    "    best = None\n",
    "    best_hits = -1\n",
    "    for cand in order:\n",
    "        hits = sum(1 for c in cand if c in present)\n",
    "        if hits > best_hits:\n",
    "            best, best_hits = cand, hits\n",
    "    if best_hits >= 2:\n",
    "        # use preferred order\n",
    "        return [x for x in best if x in present]\n",
    "    # fallback\n",
    "    return list(index_values)\n",
    "\n",
    "def make_mean_count_chart(df_in, count_col, freq_col, title):\n",
    "    agg = (\n",
    "        df_in.groupby(freq_col)[count_col]\n",
    "             .agg(mean=\"mean\", n=\"count\")\n",
    "             .sort_index()\n",
    "    )\n",
    "    order = pick_freq_order(agg.index)\n",
    "    agg = agg.reindex(order)\n",
    "\n",
    "    # plot\n",
    "    ax = agg[\"mean\"].plot(kind=\"bar\", figsize=(9,4), width=0.8, color=\"cornflowerblue\")\n",
    "    ax.set_title(title, fontsize=16, weight=\"bold\")\n",
    "    ax.set_xlabel(\"AI usage frequency\", fontsize=12)\n",
    "    ax.set_ylabel(\"Mean number of selections\", fontsize=12)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.yticks(fontsize=11)\n",
    "    plt.grid(False)\n",
    "\n",
    "    # n per bar\n",
    "    for p, n in zip(ax.patches, agg[\"n\"].astype(int).values):\n",
    "        ax.annotate(f\"n={n}\",\n",
    "                    (p.get_x() + p.get_width()/2, p.get_height()),\n",
    "                    ha=\"center\", va=\"bottom\", fontsize=9, xytext=(0,3),\n",
    "                    textcoords=\"offset points\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # print table\n",
    "    print(\"\\nTable –\", title)\n",
    "    print(agg.round({\"mean\":2}))\n",
    "\n",
    "# calculation\n",
    "df[\"reasons_count\"]  = df[COL_REASONS].apply(count_multiselect_unique)\n",
    "df[\"purposes_count\"] = df[COL_PURPOSES].apply(count_multiselect_unique)\n",
    "\n",
    "# \n",
    "sub_rea = df.dropna(subset=[COL_FREQ]).copy()\n",
    "sub_pur = df.dropna(subset=[COL_FREQ]).copy()\n",
    "\n",
    "# graph\n",
    "make_mean_count_chart(\n",
    "    sub_rea, \"reasons_count\", COL_FREQ,\n",
    "    \"Mean number of selected reasons by AI usage frequency\"\n",
    ")\n",
    "\n",
    "make_mean_count_chart(\n",
    "    sub_pur, \"purposes_count\", COL_FREQ,\n",
    "    \"Mean number of selected purposes by AI usage frequency\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56787856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "COL_FREQ     = \"Frequency use of AI_school\"    \n",
    "COL_REASONS  = \"Reasons to use AI\"      \n",
    "COL_PURPOSES = \"Purposes to use AI\"     \n",
    "\n",
    "# -------------------- Helpers --------------------\n",
    "def _nf(s: str) -> str:\n",
    "    \"\"\"Lowercase + ASCII-Folding (ä->a, ö->o, ß->ss) + nur a-z0-9.\"\"\"\n",
    "    s = str(s).strip().lower()\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"\", s)\n",
    "\n",
    "FREQ_MAP = {\n",
    "    \"immer\": \"Always\",\n",
    "    \"haufig\": \"Often\",\n",
    "    \"manchmal\": \"Sometimes\",\n",
    "    \"selten\": \"Rarely\",\n",
    "    \"nie\": None,\n",
    "}\n",
    "\n",
    "def canon_freq(x):\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    k = _nf(x)\n",
    "    return FREQ_MAP.get(k, None)  # unbekannte Labels ausschließen\n",
    "\n",
    "def count_multiselect_unique(val):\n",
    "    \"\"\"Zählt eindeutige, nicht-leere Auswahloptionen (Komma/Semikolon-getrennt).\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return 0\n",
    "    if isinstance(val, list):\n",
    "        tokens = [str(x).strip() for x in val]\n",
    "    else:\n",
    "        tokens = re.split(r\"\\s*[;,]\\s*\", str(val))\n",
    "        tokens = [t.strip() for t in tokens]\n",
    "    tokens = [t for t in tokens if t]\n",
    "    return len(set(tokens))\n",
    "\n",
    "def make_mean_count_chart(df_in, count_col, freq_col, title):\n",
    "    ORDER = [\"Always\", \"Often\", \"Sometimes\", \"Rarely\"]\n",
    "    sub = df_in.dropna(subset=[freq_col]).copy()\n",
    "    agg = (\n",
    "        sub.groupby(freq_col)[count_col]\n",
    "           .agg(mean=\"mean\", n=\"count\")\n",
    "           .reindex(ORDER)\n",
    "    )\n",
    "\n",
    "    # Plot\n",
    "    ax = agg[\"mean\"].plot(kind=\"bar\", figsize=(9,4), width=0.8, color=\"cornflowerblue\")\n",
    "    ax.set_title(title, fontsize=16, weight=\"bold\")\n",
    "    ax.set_xlabel(\"AI usage frequency\", fontsize=12)\n",
    "    ax.set_ylabel(\"Mean number of selections\", fontsize=12)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.yticks(fontsize=11)\n",
    "    plt.grid(False)\n",
    "\n",
    "    # n-Labels\n",
    "    for p, n in zip(ax.patches, agg[\"n\"].fillna(0).astype(int).values):\n",
    "        ax.annotate(f\"n={n}\", (p.get_x() + p.get_width()/2, p.get_height()),\n",
    "                    ha=\"center\", va=\"bottom\", fontsize=9, xytext=(0,3),\n",
    "                    textcoords=\"offset points\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Tabelle\n",
    "    print(\"\\nTable –\", title)\n",
    "    print(agg.round({\"mean\": 2}))\n",
    "\n",
    "# -------------------- Berechnung --------------------\n",
    "df[\"freq4_en\"] = df[COL_FREQ].apply(canon_freq)\n",
    "\n",
    "df[\"reasons_count\"]  = df[COL_REASONS].apply(count_multiselect_unique)\n",
    "df[\"purposes_count\"] = df[COL_PURPOSES].apply(count_multiselect_unique)\n",
    "\n",
    "make_mean_count_chart(\n",
    "    df, \"reasons_count\", \"freq4_en\",\n",
    "    \"Mean number of selected reasons by AI usage frequency\"\n",
    ")\n",
    "\n",
    "make_mean_count_chart(\n",
    "    df, \"purposes_count\", \"freq4_en\",\n",
    "    \"Mean number of selected purposes by AI usage frequency\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca032f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USEFULNESS X AI USAGE\n",
    "\n",
    "# ---- Fill in the total respondents per usage group (counts) ----\n",
    "totals = {\n",
    "    \"Daily\": 56,                   # <-- PUT TOTAL HERE\n",
    "    \"Several times per week\": 68,  # <-- PUT TOTAL HERE\n",
    "    \"About once per week\": 38,     # <-- PUT TOTAL HERE\n",
    "    \"Rarely\": 31                   # <-- PUT TOTAL HERE\n",
    "}\n",
    "\n",
    "# ---- Your percentage table (from the message) mapped to English labels ----\n",
    "percent = pd.DataFrame(\n",
    "    {\n",
    "        \"About once per week\": [7.9, 55.3, 31.6, 5.3, 0.0],\n",
    "        \"Several times per week\": [29.4, 42.6, 27.9, 0.0, 0.0],\n",
    "        \"Rarely\": [0.0, 32.3, 61.3, 6.5, 0.0],\n",
    "        \"Daily\": [75.0, 23.2, 1.8, 0.0, 0.0],\n",
    "    },\n",
    "    index=[\"Very useful\", \"Quite useful\", \"Neutral\", \"Slightly useful\", \"Not useful at all\"]\n",
    ")\n",
    "\n",
    "# Reorder x-axis as requested\n",
    "order_x = [\"Daily\", \"Several times per week\", \"About once per week\", \"Rarely\"]\n",
    "percent = percent[order_x]\n",
    "\n",
    "# ---- Convert percentages to counts using 'totals' ----\n",
    "totals_vec = pd.Series(totals)[order_x].astype(float)\n",
    "counts = (percent * totals_vec.values).div(100)\n",
    "counts = counts.round(0).astype(int)  # round to integers\n",
    "\n",
    "# ---- Stack order (dark → light) ----\n",
    "order_stack = [\"Very useful\", \"Quite useful\", \"Neutral\", \"Slightly useful\", \"Not useful at all\"]\n",
    "counts = counts.loc[order_stack]\n",
    "\n",
    "# ---- Colors: high = dark blue (bottom), low = light blue (top) ----\n",
    "colors = {\n",
    "    \"Very useful\": \"#08306b\",\n",
    "    \"Quite useful\": \"#2171b5\",\n",
    "    \"Neutral\": \"#59a3ce\",\n",
    "    \"Slightly useful\": \"#83b3e2\",\n",
    "    \"Not useful at all\": \"#c4d9ee\",\n",
    "}\n",
    "\n",
    "# ---- Plot counts (stacked) ----\n",
    "ax = counts.T.plot(\n",
    "    kind=\"bar\",\n",
    "    stacked=True,\n",
    "    figsize=(10, 6),\n",
    "    color=[colors[k] for k in order_stack],\n",
    "    width=0.9\n",
    ")\n",
    "\n",
    "ax.set_title(\"Usefulness of AI by AI usage (school + freetime)\", fontsize=16, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Number of respondents\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"AI usage (school + freetime)\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xticks(rotation=0, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Legend inside, ordered so dark (Very useful) at the bottom\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "lookup = {lab: h for h, lab in zip(handles, labels)}\n",
    "legend_display = order_stack[::-1]  # show top→bottom: light→dark\n",
    "ax.legend([lookup[l] for l in legend_display], legend_display,\n",
    "          title=\"Usefulness\", loc=\"upper right\",\n",
    "          fontsize=12, title_fontsize=13, framealpha=0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe7fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONCERNS X AI USAGE\n",
    "\n",
    "# ----- Daten (Counts) -----\n",
    "data = {\n",
    "    \"Daily\": [18, 31, 7],\n",
    "    \"Several times per week\": [32, 23, 13],\n",
    "    \"About once per week\": [12, 10, 16],\n",
    "    \"Rarely\": [14, 6, 11],\n",
    "    \"Never\": [3, 4, 3],\n",
    "}\n",
    "stack_order = [\"Yes\", \"No\", \"Never thought about it\"]  # ursprüngliche Reihenfolge\n",
    "df = pd.DataFrame(data, index=stack_order)\n",
    "\n",
    "# ----- Plot (gestapelt) -----\n",
    "colors = {\n",
    "    \"Yes\": \"green\",\n",
    "    \"No\": \"red\",\n",
    "    \"Never thought about it\": \"#4A90E2\"\n",
    "}\n",
    "\n",
    "ax = df.T.plot(\n",
    "    kind=\"bar\", stacked=True, figsize=(10, 6),\n",
    "    color=[colors[k] for k in stack_order], width=0.9\n",
    ")\n",
    "\n",
    "# Titel & Achsen\n",
    "ax.set_title(\"Concern about AI by AI usage (school + free time)\", fontsize=16, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Number of respondents\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"AI usage (school + free time)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# x-Ticks leicht schräg\n",
    "plt.xticks(rotation=20, ha=\"right\", fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# --- Legende invertiert (hell oben → dunkel unten) ---\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "legend_display = stack_order[::-1]  # invertiert\n",
    "lookup = {lab: h for lab, h in zip(labels, handles)}\n",
    "ax.legend([lookup[l] for l in legend_display], legend_display,\n",
    "          title=\"Concerns\", loc=\"upper right\",\n",
    "          fontsize=12, title_fontsize=13, framealpha=0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d05d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Eingabetabelle (Counts) ---\n",
    "data = {\n",
    "    \"Stört mich sehr\":               [2, 6, 8, 3, 3, 0],\n",
    "    \"Stört mich ein wenig\":          [0, 17, 26, 5, 0, 0],\n",
    "    \"Neutral / Mir egal\":            [6, 50, 47, 7, 0, 5],\n",
    "    \"Finde ich gut\":                 [2, 8, 4, 0, 0, 0],\n",
    "    \"Finde ich sehr gut\":            [0, 3, 1, 0, 0, 0],\n",
    "}\n",
    "rows = [\n",
    "    \"Sehr verlässlich\",\n",
    "    \"Eher verlässlich\",\n",
    "    \"Teils/teils\",\n",
    "    \"Wenig verlässlich\",\n",
    "    \"Gar nicht verlässlich\",\n",
    "    \"Unsicher / Ich habe keine Meinung\",\n",
    "]\n",
    "ct = pd.DataFrame(data, index=rows)\n",
    "\n",
    "# --- Mapping Reliability -> Score (1..5); \"Unsure\" wird ausgeschlossen ---\n",
    "score_map = {\n",
    "    \"Gar nicht verlässlich\": 1,\n",
    "    \"Wenig verlässlich\":     2,\n",
    "    \"Teils/teils\":           3,\n",
    "    \"Eher verlässlich\":      4,\n",
    "    \"Sehr verlässlich\":      5,\n",
    "}\n",
    "reliability_rows = list(score_map.keys())\n",
    "\n",
    "# nur Zeilen mit Scores nehmen\n",
    "ct_scored = ct.loc[reliability_rows]\n",
    "\n",
    "# n je Haltung (ohne Unsure)\n",
    "n_per_att = ct_scored.sum(axis=0)\n",
    "\n",
    "# gewichtete Mittelwerte je Haltung\n",
    "weights = pd.Series(score_map)\n",
    "mean_per_att = (ct_scored.mul(weights, axis=0).sum(axis=0) / n_per_att).astype(float)\n",
    "\n",
    "# X-Achse englisch + in gewünschter Reihenfolge\n",
    "x_map = {\n",
    "    \"Finde ich sehr gut\":   \"I really like it\",\n",
    "    \"Finde ich gut\":        \"I like it\",\n",
    "    \"Neutral / Mir egal\":   \"Neutral / I don't care\",\n",
    "    \"Stört mich ein wenig\": \"Bothers me a bit\",\n",
    "    \"Stört mich sehr\":      \"Bothers me a lot\",\n",
    "}\n",
    "order_x = [\"I really like it\", \"I like it\", \"Neutral / I don't care\", \"Bothers me a bit\", \"Bothers me a lot\"]\n",
    "\n",
    "mean_en = mean_per_att.rename(index=x_map).reindex(order_x)\n",
    "n_en = n_per_att.rename(index=x_map).reindex(order_x)\n",
    "\n",
    "# --- Plot wie im Screenshot ---\n",
    "fig, ax = plt.subplots(figsize=(12, 4.5))\n",
    "\n",
    "bars = ax.bar(mean_en.index, mean_en.values, color=\"#1f77b4\", edgecolor=\"black\")\n",
    "\n",
    "# n über die Balken schreiben\n",
    "for bar, n in zip(bars, n_en.values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "            f\"n={int(n)}\", ha=\"center\", va=\"bottom\", fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "# Titel (zweizeilig wie im Beispiel)\n",
    "ax.set_title(\"Mean Score (1–5): Reliability by approval of teachers using AI tools to prepare lessons\",\n",
    "             fontsize=16, fontweight=\"bold\", pad=10)\n",
    "\n",
    "ax.set_ylabel(\"Mean score (1–5)\", fontsize=13, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"Attitude\", fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "# y-Achse auf 1..5 „einrasten“ lassen\n",
    "ax.set_ylim(0.9, 5.1)\n",
    "ax.set_yticks([1,2,3,4,5])\n",
    "ax.tick_params(axis=\"y\", labelsize=12)\n",
    "\n",
    "# x-Labels schräg wie im Screenshot\n",
    "plt.xticks(rotation=35, ha=\"right\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322a9098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Eingabetabelle (Counts) ---\n",
    "data = {\n",
    "    \"Stört mich sehr\":               [3, 34, 56, 11, 3, 1],\n",
    "    \"Stört mich ein wenig\":          [2, 26, 19, 3, 0, 1],\n",
    "    \"Neutral / Mir egal\":            [2, 19, 9, 1, 0, 3],\n",
    "    \"Finde ich gut\":                 [3, 3, 1, 0, 0, 0],\n",
    "    \"Finde ich sehr gut\":            [0, 2, 1, 0, 0, 0],\n",
    "}\n",
    "rows = [\n",
    "    \"Sehr verlässlich\",\n",
    "    \"Eher verlässlich\",\n",
    "    \"Teils/teils\",\n",
    "    \"Wenig verlässlich\",\n",
    "    \"Gar nicht verlässlich\",\n",
    "    \"Unsicher / Ich habe keine Meinung\",\n",
    "]\n",
    "ct = pd.DataFrame(data, index=rows)\n",
    "\n",
    "# --- Mapping Reliability -> Score (1..5); \"Unsure\" ausschließen ---\n",
    "score_map = {\n",
    "    \"Gar nicht verlässlich\": 1,\n",
    "    \"Wenig verlässlich\":     2,\n",
    "    \"Teils/teils\":           3,\n",
    "    \"Eher verlässlich\":      4,\n",
    "    \"Sehr verlässlich\":      5,\n",
    "}\n",
    "reliability_rows = list(score_map.keys())\n",
    "ct_scored = ct.loc[reliability_rows]\n",
    "\n",
    "# n je Haltung (ohne Unsure)\n",
    "n_per_att = ct_scored.sum(axis=0)\n",
    "\n",
    "# gewichtete Mittelwerte je Haltung\n",
    "weights = pd.Series(score_map)\n",
    "mean_per_att = (ct_scored.mul(weights, axis=0).sum(axis=0) / n_per_att).astype(float)\n",
    "\n",
    "# X-Achse englisch + Reihenfolge\n",
    "x_map = {\n",
    "    \"Finde ich sehr gut\":   \"I really like it\",\n",
    "    \"Finde ich gut\":        \"I like it\",\n",
    "    \"Neutral / Mir egal\":   \"Neutral / I don't care\",\n",
    "    \"Stört mich ein wenig\": \"Bothers me a bit\",\n",
    "    \"Stört mich sehr\":      \"Bothers me a lot\",\n",
    "}\n",
    "order_x = [\"I really like it\", \"I like it\", \"Neutral / I don't care\", \"Bothers me a bit\", \"Bothers me a lot\"]\n",
    "\n",
    "mean_en = mean_per_att.rename(index=x_map).reindex(order_x)\n",
    "n_en = n_per_att.rename(index=x_map).reindex(order_x)\n",
    "\n",
    "# --- Plot ---\n",
    "fig, ax = plt.subplots(figsize=(12, 4.5))\n",
    "\n",
    "bars = ax.bar(mean_en.index, mean_en.values, color=\"#1f77b4\", edgecolor=\"black\")\n",
    "\n",
    "# n über den Balken\n",
    "for bar, n in zip(bars, n_en.values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "            f\"n={int(n)}\", ha=\"center\", va=\"bottom\", fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "ax.set_title(\"Mean Score (1–5): Reliability by approval of teachers using AI tools to give grades\",\n",
    "             fontsize=16, fontweight=\"bold\", pad=10)\n",
    "ax.set_ylabel(\"Mean score (1–5)\", fontsize=13, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"Attitude\", fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "ax.set_ylim(0.9, 5.1)\n",
    "ax.set_yticks([1, 2, 3, 4, 5])\n",
    "ax.tick_params(axis=\"y\", labelsize=12)\n",
    "\n",
    "plt.xticks(rotation=35, ha=\"right\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8de4b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Eingabetabellen ---\n",
    "reasons_df = pd.DataFrame(\n",
    "    {\n",
    "        1: [20, 18, 3, 1],\n",
    "        2: [12, 15, 15, 4],\n",
    "        3: [3, 14, 33, 4],\n",
    "        4: [1, 7, 13, 2],\n",
    "        5: [0, 4, 6, 2],\n",
    "    },\n",
    "    index=[\"Rarely\", \"Sometimes\", \"Often\", \"Always\"]\n",
    ")\n",
    "\n",
    "purposes_df = pd.DataFrame(\n",
    "    {\n",
    "        1: [11, 8, 1, 2],\n",
    "        2: [11, 14, 5, 1],\n",
    "        3: [7, 20, 22, 1],\n",
    "        4: [5, 14, 16, 6],\n",
    "        5: [2, 3, 15, 2],\n",
    "        6: [1, 2, 6, 1],\n",
    "        7: [0, 1, 5, 0],\n",
    "    },\n",
    "    index=[\"Rarely\", \"Sometimes\", \"Often\", \"Always\"]\n",
    ")\n",
    "\n",
    "# --- Reorder index so Always is first ---\n",
    "order_x = [\"Always\", \"Often\", \"Sometimes\", \"Rarely\"]\n",
    "reasons_df = reasons_df.reindex(order_x)\n",
    "purposes_df = purposes_df.reindex(order_x)\n",
    "\n",
    "# --- Funktion zur Berechnung von Mean Scores ---\n",
    "def compute_mean_scores(df):\n",
    "    total = df.sum(axis=1)\n",
    "    mean_score = (df.mul(df.columns, axis=1).sum(axis=1) / total).round(2)\n",
    "    return mean_score, total\n",
    "\n",
    "# --- Plotfunktion ---\n",
    "def plot_mean(mean_score, total, title):\n",
    "    fig, ax = plt.subplots(figsize=(9, 5))\n",
    "    bars = ax.bar(mean_score.index, mean_score.values, color=\"#1f77b4\", edgecolor=\"black\")\n",
    "\n",
    "    # n über Balken schreiben\n",
    "    for bar, n in zip(bars, total.values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "                f\"n={int(n)}\", ha=\"center\", va=\"bottom\", fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "    ax.set_title(title, fontsize=15, fontweight=\"bold\", pad=12)\n",
    "    ax.set_ylabel(\"Mean number of selections\", fontsize=13, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Frequency of AI usage\", fontsize=13, fontweight=\"bold\")\n",
    "    ax.tick_params(axis=\"x\", labelsize=12)\n",
    "    ax.tick_params(axis=\"y\", labelsize=12)\n",
    "\n",
    "    plt.ylim(0, max(mean_score.values) + 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Reasons → Purposes ---\n",
    "mean_reasons, n_reasons = compute_mean_scores(reasons_df)\n",
    "plot_mean(mean_reasons, n_reasons, \"Mean selection of purposes to use AI by AI usage frequency\")\n",
    "\n",
    "# --- Purposes → Motivations ---\n",
    "mean_purposes, n_purposes = compute_mean_scores(purposes_df)\n",
    "plot_mean(mean_purposes, n_purposes, \"Mean selection of motivations to use AI by AI usage frequency\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
